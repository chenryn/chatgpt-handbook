# GPTCache 介绍

ChatGPT 出现以后，业界纷纷看好大语言模型的未来，有人认为 ChatGPT 会是未来的 OS，有人认为 ChatGPT 会是未来的 UI，有人认为 ChatGPT 会是未来的 PC。但目前最大的问题是：ChatGPT 的推理性能存在瓶颈，即便付费用户，也时不时碰到超时、失败。

如果从 OS 的角度类比，LLM 本身是内核，外部理当会有一系列设施，完成内核和应用之间的保护、交互工作。这时候我们可以引入传统的系统架构设计中一个概念：性能问题都可以靠加缓存层。那么，ChatGPT 有没有可能加缓存层呢？

zilla 公司日前发布了开源项目：GPTCache，实现了类似思路下的 ChatGPT 应用层缓存。

GPTCache 项目的核心思路，是引入向量搜索引擎作为缓存层，将每次问答的内容，转为向量存储。后续提问时，将提问词在向量搜索引擎中进行一次相关性搜索，如果存储中有相关性超过阈值的结果，则直接返回搜索结果，不再请求 ChatGPT。

需要注意的是，这种方案虽然可以降低 ChatGPT 的 token 消耗，提升问答性能。但本质上是在用搜索替代 AI，会造成问答效果被削弱。因此，如何判断提问和缓存数据的相关性真的足够高，是缓存效果好坏的关键。GPTCache 项目中提供了多种不同的 `similar_evaluation` 策略供选择使用。包括：`exact_match`，`ONNX`，`distance` 等。

作为新项目，zilla 公司目前也处于摸索阶段，所以默认策略比较保守，采用的是 `exact_match` 策略。该策略是只有当本次的提问词，和缓存中的某次历史提问词一模一样的情况下，才会使用历史提问词的回答结果。否则都继续向 ChatGPT 提问。

README 中提供的另一个示例，则采用了 `ONNX` 策略。该策略同样仅使用本次的提问词，和缓存中的历史提问词做对比，不过这次稍微放开，只要 embeddings 的相似度超过 0.8 就使用匹配度最高的历史提问词的回答结果。

只有 `distance` 策略，才会将本次的提问词，和缓存中的历史提问词加回答文本的整体 embeddings 做相似度计算。这种情况缓存命中率最高，但 AI 的灵活性最低。大家可以根据自身的情况，包括问答准确度的期望、API 消费的额度等等，综合考虑，到底应该使用何种策略。


