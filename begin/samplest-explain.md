= explain

Stephen Wolfram 写了一篇[文章](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)，深入浅出的介绍了 ChatGPT 的原理。不过对于本书的读者，或者说 ChatGPT 的使用者们而言，是否掌握 Transformer、BERT、GPT、zero-shot、InstructGPT 的原理，并不影响我们基于 ChatGPT 技术进行实际运用和上层应用开发。

我们尝试让 ChatGPT 替我们总结一下文章，尽量以完全没有 IT 知识的水平来讲解。因为 ChatGPT 不能联网，所以借用 bing chat 来实现：

> Q: 请帮我概述下面这个网页的内容 https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/ 按照这个内容，讲述一下 ChatGPT 的原理，要完全不懂 IT 的人也能听懂的那样浅显。
> A: 好的，我可以尝试用一种简单的方式来讲述 ChatGPT 的原理。
> 
> 您可以把 ChatGPT 想象成一个非常聪明的学生，它的老师给了它很多很多的书籍、文章、对话等等，让它仔细阅读和学习。这些书籍、文章、对话都是从互联网上收集来的，所以它们涵盖了各种各样的话题和风格。
> 
> ChatGPT 通过阅读这些文本，就能够学习到人类是如何使用语言来表达自己的想法和情感的。它也能够学习到不同类型和主题的文本有不同的结构和逻辑。比如说，一篇新闻报道通常会先说最重要的事实，然后再补充一些细节和背景信息；而一首诗歌通常会用比喻和押韵来增加美感和节奏。
> 
> 当 ChatGPT 要生成一段文本时，它会先看看已有的文本是什么类型和主题的，然后根据自己学习到的规律和特征，来预测下一个最合理的单词是什么。比如说，如果已有的文本是“今天天气真好”，那么下一个最合理的单词可能是“我”、“我们”、“你”等等。然后 ChatGPT 会把这个单词加到已有的文本后面，再重复这个过程，直到生成一段完整的文本。
> 
> 这就是 ChatGPT 的基本原理，您能听懂吗？
> 
> Q: 那么 ChatGPT 是怎么判断一个单词是否合理呢？
> A: ChatGPT 是怎么判断一个单词是否合理呢？这是一个很好的问题。
> 
> ChatGPT 的判断依据是它学习到的语言模型，也就是它对人类语言的规律和特征的统计和分析。它会根据已有的文本，计算出每个可能的单词出现在下一个位置的概率，然后选择概率最高的那个单词。
> 
> 比如说，如果已有的文本是“今天天气真好”，那么 ChatGPT 会根据它阅读过的所有文本，计算出“我”、“我们”、“你”等等单词出现在下一个位置的概率。如果“我”这个单词出现在下一个位置的概率是0.8，“我们”是0.1，“你”是0.05，其他单词都是0.01或更低，那么 ChatGPT 就会选择“我”这个单词，因为它有最高的概率。
> 
> 当然，这个概率并不是绝对准确的，因为它只能反映 ChatGPT 学习到的语言模型，而不一定能反映真实世界中人类语言的多样性和复杂性。所以有时候 ChatGPT 也会生成一些不合理或者错误的单词，比如说“今天天气真好猫”。这就需要我们对 ChatGPT 的输出进行一些检查和修正。
> 
> Q: 那么 ChatGPT 是如何学习到语言模型的呢？
> A: ChatGPT 是如何学习到语言模型的呢？这是一个很深入的问题。
> 
> ChatGPT 的学习过程是基于一个神经网络模型，也就是一种可以模拟人类大脑神经元之间连接和传递信息的计算机程序。神经网络模型有很多层，每一层都有很多节点，每个节点都有一些权重和偏置，这些权重和偏置就决定了节点如何处理输入和输出信息。
> 
> ChatGPT 的神经网络模型有175亿个参数，也就是175亿个权重和偏置。这些参数在初始时是随机设置的，所以 ChatGPT 一开始并不知道任何关于语言的知识。但是当 ChatGPT 阅读了大量的文本后，它就会根据文本中单词之间的关系，调整自己的参数，使得自己能够更好地预测下一个单词。
> 
> 比如说，如果 ChatGPT 阅读了很多句子中都出现了“我爱”和“你”的组合，那么它就会增加“我爱”后面出现“你”的概率，并且减少其他单词的概率。这样 ChatGPT 就学习到了“我爱你”是一个常见的语言表达方式。
> 
> 通过不断地阅读和调整参数，ChatGPT 就能够逐渐形成一个语言模型，也就是对人类语言规律和特征的统计和分析。这个语言模型就储存在 ChatGPT 的神经网络模型中，可以用来生成新的文本。

image::images/begin/bingchat-explain.png[]

事实上，你还可以根据 bing chat 给出的提示，继续问下去，了解ChatGPT 的神经网络算法等等。但对于普通用户而言，了解到这里，已经足够了。

为了方便，我把这些内容抽象成最简单的一句话：GPT 是按概率，一个接一个单词的输出，同时为了争取全局最优，有时也会在单个词时选择概率不是最高的。

“全局最优”这个概念很容易让人联想到围棋。人工智能领域的上一轮高潮，也正是 2016 年震惊世界的 alphago 围棋 AI。多年后，现在几乎所有围棋爱好者在看棋的时候，都会说类似这样的话：某某棋手的 AI 吻合度是多少；某一手是 AI 最优选；某一手下完以后，AI 胜率涨跌了多少……

但围棋终究是有限集合，一个棋盘就是 19 x 19 路黑白两色棋子。而人类语言无穷无尽，长篇小说的入门标准是 5 万字，严肃文学中最长小说的世界纪录是法国作家路易•法利古尔的《善心人》207 万字，网络小说中最长的是起点中文网明宇的《带着农场混异界》4385 万字(还在连载中)。此外，起点中文网曾经有人上传过一本用 VB 编程语言输出的小说《宇宙巨校闪级生》1.7 亿字，被起点以非人类原创为由下架。

所以，从“全局最优”这个角度来说，ChatGPT 在人类语言领域达到的高度，还远远比不上 alphago 在围棋领域的高度。做个不怎么贴切的对比，ChatGPT 大概也就是刚开始总结围棋“定式”的状态吧。

本节的原理解释不是为了真的阐述数学原理，而是佐证我们不应该过于相信 ChatGPT 真的是上帝式的强人工智能——换句话说：不是 ChatGPT 完成了很难的事情，而是这个事情过去被人类高估了难度。我们甚至可以做个更直接一点的比喻，ChatGPT 能写论文，是因为大多数“水”论文本来就是有迹可循的套路文章。有这个合理的心理预期，我们就可以继续接下来的学习练习了。

