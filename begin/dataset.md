= dataset

所有人工智能算法都会分为训练和推理两步。算法的效果好坏，很大程度上取决于训练数据本身的质量。ChatGPT 所用的训练数据，openai 公司没有单独公布过细节。不过考虑到 ChatGPT 是在前序 GPT 算法基础上发展而来，我们可以侧面分析 GPT-3 的训练数据集情况。

人工智能领域知名人士 Alan D. Thompson 博士发表过一篇文章，介绍在大语言模型领域目前常用的数据集情况。其中根据 openai 论文公开的 token 数据情况，推测了 GPT-3 所用训练数据集大小一共有 753.4GB。具体分布如下：

* 维基百科: 11.4GB。维基百科是世界著名的免费、多语种、在线百科全书，有超过 30 万名志愿者在贡献内容。一般参与训练的是其中的英文版部分，包括 662 万篇文章，超过 42 亿个单词。这其中传记类占 27.8%，地理类占17.7%，文化艺术类占15.8%，历史类占9.9%，生物医学占7.8%，体育类占6.5%，工商类占4.8%，理工和数学占3.5%。
* Gutenberg Book：21GB。古腾堡书籍语料库，是电子书发明人 Michael Hart 创建的项目，也是世界上第一个免费电子书网站。网站收录了各种语言文字的书籍，有 12 种语言收录超过 50 本，中文书籍有 500 本，不过基本都是古籍。一般用于训练的是语料库中精选的 SPGC 版本。因为是在线网站，我们可以直接看到按日排列的前一百名书籍清单。比如 2023 年 3 月 10 日，排名第一个的书籍为莎士比亚的《罗密欧与朱丽叶》，而前 100 名中唯一的中文书籍，很巧合正是第 88 名汤显祖的《牡丹亭》。
* Bibliotik Journey：101GB。Bib 是互联网最大的电子书站点，通过 P2P 方式分发下载，种子数量超 50 万。 EleutherAI 实验室在 2021 年为了训练 GPT-Neo 大模型，整合精选了该电子书数据集，占EleutherAI 实验室最后使用的 Pile 数据集中全部数据的 12.07%。
* Reddit links：50GB。Reddit 是一个流行的社交媒体平台，WebText 数据集从 Reddit 平台上爬取了所有三个赞以上的出站链接的网页，代表了流行内容的风向标。
* Common Crawl：570GB。这是一个从 2011 年开始一直在爬取的数据集，包括原始网页、元数据和提取的文本，存储在 AWS 上，总量超 1PB，并以每月 20TB 的速度持续新增。一般用来训练的只是 Common Crawl 中的 C4 部分。从数据分析来看，除谷歌专利网站占 0.48% 比例偏高以外，其他来源网站的占比都比较平均，维持在0.04%一下。

openai 自身公开的训练数据分语种统计结果(<https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_word_count.csv>)中，训练数据集里英语单词占比高达 92%。此外，法语占 1.81%，德语占1.47%，其他语种均在 1%一下，汉语比例为0.1%。但实际 ChatGPT 的各语种问答能力，远超 openai 自身的预料之外。人类语言可能在某种程度上有超乎人类理解的相通。

也有其他方面的消息，称 GPT-3 的训练语料大小高达 45TB。两个数据的差距实在太大，有可能 45TB 是上述数据来源未精选之前的总大小之和。

这些数据集，能多大程度上代表整个互联网呢？www.worldwidewebsize.com 网站长期跟踪谷歌、必应等搜索引擎上可检索到的互联网总网页数量，到目前为止，总索引网页数量为 58.5 亿。还有另一份针对网页 HTML 大小的长期跟踪，目前互联网网页的平均大小为 1.2MB。估算可知，整个互联网的文本大小为 7000TB。去除掉各种 HTML 标签，按照二八法则大致去掉长尾的雷同内容，我们可以武断的认为，整个互联网上的文本大概会是 1000TB 大小。但直接运用这个 1000TB 数据训练 AI 对话，未必是最佳方案。多年前，微软小冰"学会"骂人的事故就是明证。

此外，由于 ChatGPT 的思维链能力需要刻意锻炼逻辑能力，训练数据可能还有来自 GitHub 的代码数据集、StackExchange 的编程问答数据集等。

我们可以看到，目前 ChatGPT 的训练数据，基本来自英语互联网世界，对中文互联网数据的理解有所缺失。这也是中国互联网公司巨头的一次机会。但中文互联网上也确实还缺少如此量级的、开放且标准化的数据集语料。甚至可能连对应的形态都不存在。比如：中国几乎没有 reddit、hackernews 这类以出站链接和问答评论为主的社交媒体平台。现存的中文语料库，几乎都来自各大高校和科研机构，如北京语言大学 BBC、清华大学 OpenSLR、北京大学 CCL、南京农业大学 NEPD、智源研究院 WuDaoCorpora 等。复旦大学发布 MOSS 人工智能对话机器人时，就坦言自己完全是使用英文互联网世界的标准语料，并无特殊的中文数据。

科研机构很难长期维护一份实时更新的数据集，因此这一方面依赖于中国互联网企业自身的努力，比如：百度百科、知乎问答提供优选内容，京东、当当免费电子书分发、知网免费期刊杂志公开、微信朋友圈开放出站链接、微博热搜榜及评论的整合等等。另一方面，也考量监管层的探索。中国证监会科技监管局局长姚前，日前在《中国金融》2023 年第 6 期发表署名文章《ChatGPT类大模型训练数据的托管与治理》，提出要抓住高质量数据这个"牛鼻子"，对高质量数据的供给，"要统筹兼顾自立自强和对外开放。可考虑对 Wikipedia、Reddit 等特定数据源建立过滤后的境内镜像站点，供国内数据处理者使用"。

